---
title: "q-Means: quantum algorithm for clustering"
comments: true 
status: done
tags: 
- qml-algo
- algorithm
description: "This is a quantum algorithm for clustering unsupervised data stored in the QRAM"
author:
- Alessandro Scinawa Luongo
permalink: qmeans
---

This post is organized as such. First, we give a brief introduction to k-means, the well-known classical algorithm for clustering. We then introduce $\delta$-k-Means: a noise-resistent version of k-means which is best suited to be quantized. We show how to find the same model fitted by $\delta$-k-means with a quantum computer, with a runtime that depends only logarithmically on the number of data points. All the tools used in the quantum algorithm are followed by a brief description, or a link to the relevant page or the relevant [paper](https://arxiv.org/abs/1812.03584).

## Clustering and k-menas

The $k$-means is one of the most simple and useful algorithm for clustering. It is an iterative algorithm, introduced first in {% cite lloyd1982least %}. Clustering is an unsupervised problem in machine learning, where we want to partition our dataset in $k$ clusters: groups of points that are deemed similar. In k-means, the similarity measure is as simple as the Euclidean distance between points. The inputs of $k$-means algorithm is an unsupervised dataset: $v_{i} \in R^{d}$ for $i \in \[n\]$. The output of the $k$-means algorithm is a list of $k$ cluster centers $c_1 \cdots c_k$, which are often called _centroids_. 

The k-Means algorithm starts by selecting $k$ initial centroids as random vectors of the training set, or by using efficient heuristics like the $k$-means++ {% cite arthur2007k %}. It then alternates between two steps: (i) Each data point is assigned the label of the closest centroid. (ii) Each centroid is updated to be the average of the data points assigned to the corresponding cluster. These two steps are repeated until convergence, that is, until the change in the centroids during one iteration is sufficiently small. More precisely, we are given a dataset $V$ of vectors $v_{i} \in R^{d}$ for $i \in \[n\]$. At step $t$, we denote the $k$ clusters by the sets $C\_j^t$ for $j \in [k]$, and each corresponding centroid by the vector $c_{j}^{t}$. At each iteration, the data points $v\_i$ are assigned to a cluster $C_j^t$ such that $C_1^t \cup C\_2^t \cdots \cup C\_K^t = V$ and $C\_i^t \cap C\_l^t = \emptyset$ for $i \neq l$. Let $d(v_{i}, c\_{j}^{t})$ be the Euclidean distance between vectors $v\_{i}$ and $c\_{j}^{t}$.  
The first step of the algorithm assigns each $v_{i}$ a label $\ell(v_{i})^t$ corresponding to the closest centroid, that is 
$$\ell(v_{i})^{t} = \text{argmin}_{j \in [k]}(d(v_{i}, c_{j}^{t})).$$ The centroids are then updated as $c\_{j}^{t+1} = \frac{1}{ |C\_{j}^t|} \sum\_{i \in C\_{j}^t } v\_{i},$
so that the new centroid is the average of all points that have been assigned to the cluster in this iteration. We say that we have converged if for a small threshold $\tau$ we have that
$$\frac{1}{k}\sum_{j=1}^{k}{d(c_{j}^{t},c_{j}^{t-1}}) \leqslant \tau. $$ 
In this case, the loss function that this algorithm aims to minimize is the RSS (residual sums of squares), the sum of the squared distances between points and the centroid of their cluster. 

$$ \text{RSS} := \sum_{j \in [k]}\sum_{i\in C_j} d(c_j, v_i)^2 $$

The RSS decreases at each iteration of the $k$-means algorithm, the algorithm therefore converges to a local minimum for the RSS. The number of iterations $T$ for convergence depends mostly on the data and the number of clusters.  A single iteration has complexity of $O(knd)$ since the $n$ vectors of dimension $d$ have to be compared to each of the $k$ centroids. The number of iteration needed to reach converncence are usually small. 20 is a good upper bound, but with a good heuristics for selecting the initial centroids, we might converge even in a couple of iterations. 

##  $\delta$-k-means
We now consider a $\delta$-robust version of the $k$-means in which we introduce some noise. The noise affects the algorithms in both of the steps of k-means: label assignment and centroid estimation. Having a robust version of classical algorithms often proves to be very useful in quantum computing. As you know, quantum algorithms are a special kind of randomized algorithms, so we need prove that the calculation we do in order to fit the model are resistent to noise. 

Recall that with $c^{*}\_i$ we define the closest centroid to the data point $v\_i$. In this case, given a threshold $\delta$, the set of possible labels $L\_{\delta}(v\_i)$ for $v\_i$ is 
defined as follows:

$$L_{\delta}(v_i)  =  \{c_p  : \| d^2(c^*_i, v_i ) - d^2(c_p, v_i) \| \leq \delta \: \}$$ 

The assignment rule of $\delta$-k-means selects arbitrarily a cluster label from the set $L\_{\delta}(v\_i)$.  

As second source of noise, we add $\delta/2$ noise during the calculation of the centroid. Let $\mathcal{C}\_j^{t+1}$ be the set of points which have been labeled by $j$ in the previous step. For $\delta$-k-means we pick a centroid $c^{t+1}_j $ with the property that:

$$ \norm{ c^{t+1}_j - \frac{1}{\|\mathcal{C}^{t+1}_j\|}\sum_{v_i \in \mathcal{C}^{t+1}_j} v_i} < \frac{\delta}{2}. $$

One way to do this is to calculate the centroid exactly and then add some small Gaussian noise to the vector to obtain the robust version of the centroid.  Note that for a data that is expected to be clusterable (i.e. balls of points, that you can roughly think as univariate gaussians centered in $c_j$) the number of vectors on the boundary that risk to be misclassified in each step ( that is the vectors for which $\|L\_{\delta}(v\_i)\|>1 $) is typically much smaller compared to the vectors that are close to a unique centroid. Second, we also increase by $\delta/2$  the convergence threshold from the $k$-means algorithm. All in all, $\delta$-$k$-means is able to find a clustering that is robust when the data points and the centroids are perturbed with some noise of magnitude $O(\delta)$. 

## The q-means algorithm.

The quantum algorithm essentially go throught the same steps of the classical algorithm, alternatig distance calculation and centroid estimation. The tools used in the algorithms are the usual suspect. 
- Amplitude Ampification
- The [distance evaluation procedure](distances).
- Quantum linear algebra subroutines ([QBLAS](QBLAS)).
- Tomography.
- The [QRAM](QRAM).

Let's go through the most salient steps of the quantum algorithms. As the classical algorithm, we use an heuristic to select the initial centroids at time $t=0$. We store in the QRAM the dataset $V$ along with our current guess of the centroid $C \in \mathbb{R}^{k \times d}$.

As in the classical algorithm, the first step consist in choosing a set of centroids $c_0 \cdots c_k$ that we store in the QRAM (this costs $O(kd\log kd)$). Then, we use the [distance evaluation procedure](distances) to perform the following mapping :

$$\ket{i}\ket{j} \mapsto \ket{i}\ket{j}\ket{\overline{d(v_i, c_j)}}$$

where $\overline{d(v_i, c_j)}$ is the $\epsilon$-close approximation of the distance between the vector and the centroid. 


When we say "finding the minimum" we mean a very simple circuit. 
##### Lemma: Circuit for finding the minimum.
Given $k$ different $\log p$-bit registers $\otimes_{j \in [k]} \ket{a_j}$, there is a quantum circuit $U_{min}$ that maps
	$(\otimes_{j \in [p]} \ket{a_j})\ket{0} \to (\otimes_{j \in [k]}\ket{a_j})\ket{\text{argmin}(a_j)}$ in time ${O}(k \log p)$. 

We append an additional register for the result. We then repeat the following operation for $2\leq j \leq k$, we compare the first register and $j$-th register. If the value in register $j$ is smaller we swap registers $1$ and $j$ and update the result register to $j$. The cost of the procedure is ${O}(k \log p)$. The cost of finding the minimum is $\widetilde{O}(k)$ in step 2 of the $q$-means algorithm. Remember that we need to uncompute the distances by performing Step 1 in reverse. Once we apply the previous steps, we get.

$$\ket{\psi^t} := \frac{1}{\sqrt{N}}\sum_{i=1}^{n} \ket{i} \ket{ \ell^t(v_{i})}$$

This state represent the superposition of indices that belongs to the same cluster at a given time $t$. Now we want to compute the centroids. Let's see how a simple observation can guide our intuition for the next step. 

##### Claim 
Let $\chi_{j}^{t} \in \mathbb{R}^{N}$ be the scaled characteristic vector for $$\mathcal{C}_{j}$$
at iteration $t$ and $V \in \mathbb{R}^{n\times d}$ be the data matrix, then $c_{j}^{t+1} = V^{T} \chi_{j}^{t}$.

Proof: _The proof is very simple. The $k$-means update rule for the centroids is given by:_
$$c_{j}^{t+1} = \frac{1}{ |C_{j}^{t}|} \sum_{i \in C_{j} } v_{i}.$$ 
_As the columns of $V^{T}$ are the vectors_ $v_{i}$, _this can be rewritten as_ $c_{j}^{t+1} = V^{T} \chi_{j}^{t}$.

This tells us that using the [quantum linear algebra procedures](QBLAS), is possible to create the state 
$$\ket{c_j}$$

Once we have a procedure to create $\ket{C}$, we can use efficient algorithm for performing tomography. Tomography allow us to recover classically the amplitude of the state, and thus recover the centroids. Once the centroids are recovered, we can check if the new centroids moved significantly with respect to the previous iteration, that is we check if:

$$\sum_{i=0}^k | c^{t}_i - c^{t-1} | < \tau $$

If the centroid are not sensibly changed during one iteration of k-means, it means that we have reached a good-enough guess for the centroid of the cluster. 

The algorithm is formalized as follow.

##### Require 

Data matrix $V \in R^{n \times d}$ stored in [QRAM](qram) data structure. Precision parameters $\delta$ for $k$-means, error parameters
$\epsilon_1$ for distance estimation, $\epsilon_2$ and $\epsilon_3$ for matrix multiplication and $\epsilon_4$ for tomography. 
##### Ensure
Outputs vectors $c_{1}, c_{2}, \cdots, c_{k} \in R^{d}$ that correspond to the centroids at the final step of the $\delta$-$k$-means algorithm.\\
- Select $k$ initial centroids $c_{1}^{0}, \cdots, c_{k}^{0}$ and store them in QRAM data structure. 
- $t=0$\\
REPEAT 
1. **Step 1: Centroid Distance Estimation**
Using Theorem [distances](distances) do the following mapping: 
$\frac{1}{\sqrt{N}}\sum\_{i=1}^{n}   \ket{i} \otimes\_{j \in \[k\]} \ket{j}\ket{0} \mapsto \frac{1}{\sqrt{N}}\sum\_{i=1}^{n}  \ket{i} \otimes_{j \in \[k\]} \ket{j}\ket{\overline{d^2(v\_{i}, c\_{j}^{t})}}$
where $\|\overline{d^2(v\_{i}, c_{j}^{t})} -  d^2(v\_{i}, c\_{j}^{t}) | \leq \epsilon_{1}. $
2. **Step 2: Cluster Assignment**
Find the minimum distance among $\{d^2(v_{i}, c_{j}^{t})\}_{j\in [k]}$, then uncompute Step 1 to create the superposition of all points and their labels
$$\frac{1}{\sqrt{N}}\sum_{i=1}^{n}  \ket{i} \otimes_{j \in [k]} \ket{j}\ket{\overline{d^2(v_{i}, c_{j}^{t})}}
 \mapsto \frac{1}{\sqrt{N}}\sum_{i=1}^{n} \ket{i} \ket{ \ell^t(v_{i})}$$
3. **Step 3: Centroid states creation**
  - Measure the label register to obtain a state 
  
  $$\ket{\chi_{j}^t} = \frac{1}{\sqrt{|\mathcal{C}^t_{j}|} }  \sum_{i \in \mathcal{C}^t_j}\ket{i}$$
  
   with probability $\frac{|\mathcal{C}^{t}_j|}{n}$  
  - Perform matrix multiplication (using the [Theorem for QBLAS](qblas)) with matrix $V^T$ and vector  $\ket{\chi_{j}^t}$  to obtain the state $\ket{c_{j}^{t+1}}$ with error $\epsilon_2$, along with an estimation of $\norm{c_{j}^{t+1}}$ with relative error $\epsilon_3.$
4. **Step 4: Centroid Update** 
  - Perform tomography for the states $\ket{c_{j}^{t+1}}$ 
    with precision $\epsilon_4$ using the operation from Steps 1-3 and get a classical estimate of 
    $\overline{c}_j^{t+1}$ for the new centroids such that 
    $$|c_j^{t+1} - \overline{c}_j^{t+1}| \leq \sqrt{\eta}(\epsilon_3+\epsilon_4) = \epsilon_{centroids}$$
  - Update the QRAM data structure for the centroids with the new vectors $\overline{c}^{t+1}_0 \cdots   \overline{c}^{t+1}_k$. 
- t=t+1
- UNTIL convergence condition is satisfied. 

## Error analysis and runtime analysis
The error analysis of q-means needs to take into account all the error introduced in the quantum procedure. 


For the $q$-means to output a cluster assignment consistent with the $\delta$-$k$-means algorithm, we require that: 
$$\forall j \in [k], \quad | \overline{d^2(c_j,v_i) } - d^2(c_j,v_i)  | \leq \frac{\delta}{2}$$

The first step estimates the square distances between all points and all centroids. The error in this procedure is of the form:
$$ |\overline{d^2(c_j,v_i)} - d^2(c_j,v_i) | < \epsilon_1.$$
for a point $v_i$ and a centroid $c_j$. Thus we need to take $\epsilon_1 < \delta/2$. 

After the cluster assignment of the $q$-means (which happens in superposition), we update the clusters, by first performing a matrix multiplication to create the centroid states and estimate their norms, and then a tomography to get a classical description of the centroids.
The error in this part is $\epsilon_{centroids}$ is

$$\norm{\overline{c}_{j} - c_j} \leq \epsilon_{centroid}  = \sqrt{\eta} (\epsilon_3 + \epsilon_4).$$ 

Again, for ensuring that the $q$-means is consistent with the classical $\delta$-$k$-means algorithm we take
$\epsilon_3 < \frac{\delta}{4\sqrt{\eta}}$ and $\epsilon_4 < \frac{\delta}{4\sqrt{\eta}}$. Note also that we have ignored the error $\epsilon_2$ that we can easily deal with since it only appears in a logarithmic factor.

Combining the cost of all the steps of the algorithm (tomography, matrix multiplication, distance calculation), we get a runtime of:

$$\widetilde{O} \left( kd \frac{1}{ \epsilon_4^{2}}  \kappa(V) \left(  \mu(V) +  k \frac{\eta} { \epsilon_1} \right) + k^{2} \frac{ \eta }{\epsilon_3 \epsilon_1 } \kappa(V) \mu(V) \right) $$

From the error analysis we know that we can set $\epsilon_1 = \delta/2$, $\epsilon_3 = \frac{\delta}{4\sqrt{\eta}}$ and $\epsilon_4 = \frac{\delta}{4\sqrt{\eta}}$. 
Substituting these values in the above running time, it follows that the running time of the $q$-means algorithm is:

$$\widetilde{O} \left(  k d  \frac{\eta}{\delta^2} \kappa(V) \left( \mu(V) + k \frac{ \eta}{\delta} \right) + k^{2}\frac{ \eta^{1.5} }{ \delta^2} \kappa(V) \mu(V) \right).$$



## Well-clusterable datasets
One might what are the performances of this algorithm on real dataset. In this context, we expect to apply q-means only on dataset that are expected to be clusterable. It does not makes sense to apply a cluster algorithm if clustering is not a good model for the dataset, then you should use another tool. For instance, if you see that a $\ell_2$ distance does not capture your idea of similarity between points in an unsupervised problem, probably k-Means is not the answer you are looking for. Therefore, we are only interested in matrices that "makes sense" to cluster. We say that our dataset is apt for a clustering algorithm if: 
- Cluster centroids are well separated one from each other.
$$d(c_i, c_j) \geq \xi \quad \forall i,j \in[k]  $$
- Points that belongs to a cluster, usually are close to the centroids: at least $\lambda n$ points $v_i$ in the dataset satisfy $$d(v_i, c_{l(v_i)}) \leq \beta$$ where $c_{l(v_i)}$ is the centroid nearest to $v_{i}$. 

It turns out that these dataset have very nice properties, that have some implication in the runtime of the quantum algorithm. As consequence of this they have:
- A low Frobenius norm of $V$, which is proportional to $\sqrt{k}$ ($\norm{V}_F = O(\sqrt{k})$).
- A good distribution of singular values, such that we can take a threshold on the condition number (i.e. using $\frac{1}{\tau}$ instead of $\frac{1}{\sigma_{min}})$ as parameter in the running time without loosing too much accuracy in the classification.

These facts makes the runtime of our quantum algorithm even faster. In fact, it is possible to see that the runtime is: 

$$\widetilde{O}\left( k^2 d \frac{\eta^{2.5}}{\delta^3} + k^{2.5} \frac{\eta^2}{\delta^3} \right)$$ 

per iteration, where remember that $\eta$ is definde as the maximum norm of the vectors scaled: $1\leq \norm{v_i}^2 \leq \eta$$.


Now that you know how to build your clustering algorithm with a quantum computer, you might want to test it on real data. Once implemented, it is ready to be [evaluated](evaluate). 


## References

{% bibliography --cited %}


